{
  
    
        "post0": {
            "title": "How to read Machine Learning and Deep Learning Research papers",
            "content": "Introduction . How to read a research paper, is probably the most important skill which any one who is into research or even anyone who wishes to be updated in the field with latest advancements has to master. When someone thinks of starting out in a domain, the first advice that comes is to look for relevant literature in the domain and read papers to develop an understanding of the domain. Papers are the most reliable and updated source of information about a particular domain. A research paper is a result of days of brainstorming of ideas, and structured and systematic experimentation to express an approach. . But why is reading papers considered such an important skill to be learnt ! Why is even reading papers necessary. Let’s take on some motivation as to why is reading papers important to keep-up with the latest advances. . This article is the summary of a talk that I delivered for the Introductory Paper Reading Session generously supported by Weights and Biases whose recorded version can be found here and slides can be found here. . Dynamically Expanding field of Deep Learning . The field of deep learning has grown very rapidly in the recent years. We can quantify growth in a field by theh number of papers that come up everyday. Here is an illustration from one of the studies by ArXiv which is one of the platform where almost all of the papers, whether published or unpublished are putup. . Fig.1 - Average. No. of machine learning papers uploaded to archive every month . From the figure we can see that the average no of papers has grown to 5X averaging from 300 papers per month in 2017 to around 1500 papers per month in 2019. The figure would probably be close to or above 2k papers per month in 2021. This is a huge number of papers coming up everyday. This shows how dynamic the field is at the current time and it is just growing exponentially in terms of number of papers and amount of new ideas and experiments coming up everyday. . Let’s look at another figure from another study by arXiv . Fig.2 - Growth of Computer Science Papers on arXiv . From the figure, the number of papers in the field of Computer Science has grown like a step exponential curve and we see that around 36k papers come out each year out of which around 24k of them as we saw in the previous section are in the field of ML and DL. We can also see in both the figures that the DL field in Green and CV in yellow are among the dominant areas in terms of percentages of papers coming out every year since the early 2000s while the field of CV has grown and opened up a lot after 2012 probably when the prominent work on Image classification by deep networks showed significant performance. These studies definitely speak how fast the field of computer Science is growing and amongst it, how the sub areas related to Machine Learning and Deep Learning are evolving too. . I hope these give a good idea of how fast the field has been evolving and would continue to evolve even faster in the future. But in this fast evolving field, How can we keep up with the pace and develop a expertise in the field ? . Quoting Dr. Jennifer Raff, . ” To form a truly educated opinion on a scientific subject, you need to become familiar with current research in that field. And to be able to distinguish between good and bad interpretations of research, you have to be willing and able to read the primary research literature for yourself. “ . Why to read research Papers . To have a better grasp and understanding of the field: For a particular field, there may be a lot of video lectures and books but with the rate at which the field has been growing, no book or video lecture can accomodate the latest information as soon as they get published. So research papers provide the most updated and reliable information in the field. | To be able to contribute to the field in terms of novel ideas: When we start working in a field, the first thing that we are advised to do is to do an extensive literature survey, going through all of the latest papers that have come up in the field till date. That is advised because we can have a very good understanding of the directions of works in the field and how the people actively working in the field are thinking by reading papers. Only then we can start coming up with our own ideas to experiment upon. | To develop confidence in the field: Once we start learning about the latest works in the field and we start to develop a good understanding by performing a extensive literature survey, we start developing more confidence to perform more experiments and exploring deeper in the field. | Most condensed and authentic source of latest knowledge in the field: A reseach paper comes out of days and months, or some times even years of brainstorming of ideas, performing extensive experiments and validating the expected outcomes. The condensed experiments and thoughts is what is best expressed in a research paper that the authors write. Any new content that comes in the field in terms of state-of-the-art works is through research papers. Research papers are the source through which works that push the limits of knowledge in a field come up. | . Motivated enough ? . Now that we have attained enough motivations as to why we should read research papers, lets look at how to do literature survey in a domain. . Let’s do it ! . Literature survey of a domain . The basic steps to perform literature survey in a field are the following: . Assemble collections of resources in the form of research papers, Medium articles, blog posts, videos, GitHub repository etc. | Conduct a deep dive to classify the relevant and irrelevant material. | Take structured notes that summarises the key discoveries, findings and techniques within a paper. | We shall take Pose Estimation as a example domain and understand each step. . Step 1: Assembling all available resources . First of all we collect all the resources in the form of blog posts, github repositories, medium articles and research papers available in the field, for our case it’s pose estimation. The important question here is, where can we find relevant resources in the field ? . Following are sources where we can find the latest papers and resources: . Twitter: We can follow top researchers, groups and labs actively working and publishing in our field of domain and be updated with what they are currently working on. | ML subreddit | arXiv: Platform where almost all of the papers be it accepted to a conference or not, are uploaded. | Arxiv Sanity Preserver: Created by Anderj Karpathy which used ML techniques to suggest relevant papers based on previous searches and interests. | Papers With Code: Redirects to the paper’s abstract page on arXiv, open source implementation of the papers along with links to datasets used and a lot of other analysis and meta information like the current state-of-the art method, comparision of performance of all previous methods in the field e.t.c. | Top ML, DL Conferences (CVPR, ICCV, NeurIPS, ICML, ICLR etc): Proceedings of the following conferences are a great place to look for latest accepted works in the domains accepted by the conference. | Google Search | . Once listed down all the papers that we wish to look at and all resources we could find be it relevant or irrelevant, a table of this format shown in figure 3 can be prepared and in the first column, all the resources collected can be listed down. . Fig.3 - Initial Literature Survey chart for Pose Estimation . Step2 - Filtering out relevant and Irrelevant resources . Once listed down all the resources and prepared a table like the one shown in figure 3, the next step is to keep the relevant resources and reject the un-necessary ones which may not be directly related to what we want to work on our our research objectives. Follow the following steps to do that: . For all the resources listed down, finish 10% of reading of each resource or research paper(first pass reading, we will discuss about it later). If we find it not related to our research objective, we can reject it. . | If that resurce is related to our objective and is relevant and important to us, do a complete full pass reading over the paper. From the references, if we find any other relevant reference then mark those in the original paper and add them to the list and repeat the same over this new paper or resource now. . | So after this, this is what the final table might look like this, . Fig.4 - Table after completion of Literature Survey chart for Pose Estimation . Notice that the 2nd, 4th and 6th resources were important and relevant so we read it in detail but the other oned were not very important or the entire thing was not relevant so we read through some portion of each, whatever was necessary and left the rest. . Such a table can be really useful when we return back to it after some months or years to look for or recall what we have read or the papers we have already looked at and rejected. It helps us to save a lot of time iterating over unnecessary resources and helps us effectively dedicate time to the useful resources. . Step3: Taking Systematic Notes . Once decided on which papers to read, this step depends on the individial about they want to go about taking notes. I personally follow a annotation tool to annotate different sections of the paper according to my comfort. I prepare some flow charts for the entire flow of the paper, write some explaining notes on the paper and summarise each paper to the best of my understanding to a github repository. Here I would Like to give a shoutout to Akshay Uppal who had generously shared his blogpost with his annotated version of the MLP Mixer paper for the Weights and Biases paper reading group. I also wish to share one of my repositories of literature survey when I started working on the field of face spoofing. . Tip: You can use your own ways of making yourself comfortable with the content and taking notes either on github, notion or google docs e.t.c to organise notes. . Organization of a Paper . The majority of papers follow, more or less, the same convention of organization: . Title: Hopefully catchy ! Includes additional info about the authors and their institutions. | Abstract: High level summary of the entire work of the paper. | Introduction: Background info on the field and related research leading up to this paper. | Related works: Describe the already existing literature on the particular domain. | Methods: Highly detailed section on the study that was conducted, how it was set up, any instruments used, and finally, the process and workflow. | Results: Authors talk about the data that was created or collected, it should read as an unbiased account of what occurred. | Discussions: Here is where authors interpret the results, and convince the readers of their findings and hypothesis. | References: Any other work that was cited in the body of the text will show up here. | Appendix: More figures, additional treatments on related math, or extra items of interest can find their way in an appendix. | Finally coming to the most awaited section of the blogpost ! . How to read a Research Paper . Now that we know about the different sections of a paper, to understand how to read a paper, we need to understand how a author writes a paper. The intension of an author writing a paper is to get it accepted at a conference. In conferences, reviewers read all the submissions and take a decision based on the work and the scope and expectations of the conference. Let’s have a quick understanding of how the review process works at a very high level. . Warning: Reading a paper sequentially one section after another is not a good option. . In most of the top conferences, there are two submission deadlines: one, the abstract submission deadline. Second, the actual paper submission deadline. So why exactly are there 2 deadlines ? A separate deadline for abstract even before the actual paper deadline definitely implies that abstract is an important part of the paper. But Why is abstract important ? . Note: While Considering to submit for a conference, always note they have 2 deadlines: One, for abstract submission. Second, for the full paper submission. . Every year, a lot of papers get submitted to each conference. The number of submissions are in tens of thousands and it is not feasible to read through all the papers irrespective of how many reviewers the conference can have. So to make the review process easier and quicker, there is a guideline how different sections of a paper must be written and the reviewer also reads in that same pattern. . The first level of review is always the abstract filtering. The abstract is supposed to summarise the entire work briefly and it should clearly state the problem statement and the solution very briefly. If the abstract doesnot satisfy these criterias, the paper gets rejected in this filtering. So the abstract should clearly expain the gist of the work. Hence while reading paper too, the abstract is the place where we can find the gist of the paper clearly and briefly. Hence the abstract is read first to get an overall idea of the entire work. The authors also spend a lot of efforts in getting one figure which gives a visual illustration of the entire approach or a complete flow chart of the entire work. Even this figure contains a gist of the entire method of the paper. The authors try to condense and pack of information about thier work in a single figure. . Note: The abstract is one of the most important sections in a paper and it explains the entire gist of the paper in brief and the most important figure summarises the method adopted. . The reviewers then read the introduction section as it should explain the problem statement in a detailed way and the main proposal of the paper and the contributions. Immediately after this section, once you know what the paper is assuming, the conclusion section tells about the conclusion of the work and whether the assumptions and expectations presented in the introduction are satisfied or not. . Note: The introduction section is supposed to explain the problem statement in detail and the major contributions of the paper. We get to know the intent of the author from this section. The Conclusion section validates the assumptions and propositions given in the introduction through experiments and proofs. . After validating that the assumed propositions have been validated successfully, the method section is seen in detail to see what approach was taken to acheive the goal. In the discussion section, the experiments are explained as to why exactly the proposed method works. This is basically how a reviewer reads a paper and it is the same approach that is to be taken by a reader like us to read a paper. . 3 pass approach to read a research paper . A 3 pass approach is taken to read research papers. The content covered in each passes is in sync with the discussion on the review procedure from last section. Following are the 3 passes: . First Pass: Read the title, abstract, subsection titles and glance the figures and figure captions. Should be able to answer the five C’s (Category, Context, Correctness, Contribution, Clarity) | . | Second Pass: Read the Introduction, Conclusion and rest figures and skim rest of the sections(ignoring the details such as mathematical derivations proofs e.t.c.). | Third Pass: Reading the entire paper with an intention to reimplement it. | Lets go into detail of each section. . First pass . The main intension in the first pass is to understand the overall gist of the paper and have a bird’s eye view of the paper. The intension is to get into the authors intent about the problem statement and his thought process to develop a solution to it. The major sections which should be focused in this pass are the Abstract and the summarising figure and extract the beat possible information of the problem statemant the paper is addressing, solution and the method. The following points are what we cover in the first pass: . Read through the Title, abstract and the summarising figure. | Skip all other details of the paper. | Glance at the paper and understand its overall structure. | Try to answer the 5 Cs: After the first pass, we should be able to answer the folling 5 things about a paper: Category: Which category of paper is it, whether its an architecture paper, or a new training strategy, or a new loss function ar is it a review paper e.t.c. | Context: What previous works and area does it relate to. E.g - while Reading the DenseNet paper, it falls in the context of architectural papers and it falls into the resnet kind of networks architecture context. | Correctness: How correct and valid is the problem statement that the problem is addressing and how correct does the proposed solution sound. Honesty this can’t be totally jugded from just the first pass completely as a complete answer and unserstanding of correctness would need looking at the conclusion section, but try to judge as best to your knowledge about the correctness. | Contribution: What exactly is the contribution of the paper to the community. Eg - the resnet paper contributed the resisual block and skip connection architecture. | Clarity: How clearly does the abstract explain the problem statement and their approach towards it. | | Based on our understanding of first pass, we decide weather to go forward or stop with the paper for a detailed study into further passes. | . While discussing about literature survey, I mentioned about the 10% study on each resource to figure out if that resource is relevant to us. The 10% basically meant doing a first pass over all the resources. . Note: After the first pass, we understand the gist of the paper and get into the intent and thinking of the author. . Second Pass . After getting an overall gist of the paper after the first pass, we headon to the 2nd pass of the paper. The main intention of this section is to understand the paper in a litle more detail in terms of understanding the problemstatement in detail, validating if the paper validates the propositions it made to solve, understand the method in detail and understand the experiments well through the discussion section. The following is what we do in a 2nd pass: . Reading more in depth through the Introduction, conclusion and other figures. | Literature survey, Mathematical derivations, proofs etc and any thing that seems complicated and needs extended study from the references or other resources are skipped. | Understand the other figures in the paper properly, develop intuition about the tables, charts and analysis presented. These figures contain a lot of latent information and explain a lot more things. so it is important to extract the maximum understanding from the figures | Discuss the gist of the paper and main contents with a friend or colleague. | Mark relevant references that may be required to be revisited later. | Decide weather to go forward or stop based on this pass. | . After the 2nd pass, we have a good understanding of the paper in terms of the method of the paper, experiments and conclusions out of them. Depending on understanding from it, we go on to the next pass. . Tip: A second pass is suitable for papers that you are interested but not from your field or is not directly related to your research goal. . Third Pass . After getting a more indepth understanding of the paper after a second pass, we go on to the final pass of reading which is the most detailed pass over the paper. This pass is only for papers which are most important for the research objective and are directly related to the objective we are working on. Following are the key points for a third pass: . Reading with an intention to reimplement the paper. | Consider every minor assumption and details and make note of it. | Recreate the exact work as in the paper and compare it with original work | Identify, question and Challenge every assumption in the paper. | Make a flow chart of the entire process considering each step. | Try deriving the mathematical derivations from scratch. | Start looking at the code implementation of various components if an open source implementation is available else try to implement it. | . After a third pass, we should be knowing the paper inside out including every minor assumption and detail in it along with a clear understanding of the implementation and good understanding of the hyperparameters of each experiment perform and presented in the paper. After all the passes we can claim to have a clear understanding of the research paper. . To validate our understanding of the paper, there are a few generic question we can try to answer about the paper and if we are able to answer these questions, we have more or less understood the paper to a level where we can use it for our own research as per our requrement and our objective. . Important Questions to answer . What problems statement does the paper address? Answer to this can be found in brief in the abstract section and in detail in the Introduction section. | . | Is the problem statement a relevant one? self assessment of the problem statement. | . | What do the authors of the paper aim to accomplish, or perhaps want to achieve? Answer to this can be found from the Introduction section. | . | If a new approach/technique/method was introduced in a paper, what are the key elements of the newly proposed approach? Answer to this can be found from the Introduction section section in the contributions section and also the methoda section. | . | What is the main approach in the paper, what experiments have the authors performed and how well do the experiments results validate the conclusions? Answer to this is the entire method sections and discussions section. | . | What are the main conclusions of the paper? Answer to this is the entire conclusion section. | . | A few personal questions: What content within the paper is useful to you? Many a times a paper has many key elements which they put together to solve their problem statement. At times your problem statement maybe just a subset of the papers problem set or viceversa or a particular element of the paper may be solving some problem youa re interested into and not the others. So it is important to figure out what part of the paper is useful to you. | . | What other references do I need to or want to follow? Some sections of the paper may seem complicated or you may need to look at some previous references to understand this work completely. Also you might find some papers from the citations which are also useful to your research. So figure out the necessary references and refer to them. | . | . | . Being able to answer all these question to the ebst of our understanding and abilities validates our level of understanding of the paper. These questions can also be attempted after the 2nd pass itself and we can check our understanding after the 2nd pass itself. Then again try to answer them after a 3rd pass and judge if our understanding has improved over the 2nd pass or another pass with deeper exploration is again needed. . Tip: Nothing teaches better than implementing the entire thing from scratch and experimenting and comparing the results with original results. Even if a open source implementation is available, experimentation with the opensource code and coming up with own tweeks to the code, running different hyperparameters can improve our understanding a lot. . Conclusion . Finishing with a important note that reading papers is a skill that can be learnt with consistency over a long period of time. It is not a sprint but a marathon and demands lot of patience and consistency. . I hope I have been able to justify the title of the blog post and explain everything in detail about how to do literature survey of a domain and how to read an ML / DL research paper. Incase I missed out on anything or you have any other comments, reach me out @SaiAmritPatnaik . Thank you ! . References . Andrew Ng’s lecture in CS230 on how to read research papers | S. Keshav’s paper on how to read research papers | Slides of the talk | Blog Post 1 on reading Papers | Blog Post 2 on reading Papers |",
            "url": "https://saiamrit.github.io/technical-blog/research/reading_papers/2021/07/31/read-papers.html",
            "relUrl": "/research/reading_papers/2021/07/31/read-papers.html",
            "date": " • Jul 31, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Cross Entropy Demistified",
            "content": "Understanding “Entropy”, “Cross-Entropy Loss” and “KL Divergence” . Classification is one of the preliminary steps in most Machine Learning and Deep learning projects and Cross-Entropy Loss is the most commonly used loss function, but have you ever tried to explore what is cross-entropy or entropy exactly. . Ever imagined why cross-entropy works for classification? . This series of articles consisting of 2 parts is designed to explain in detail the intuition behind what is cross-entropy and why cross-entropy has been used as the most popular cost function for classification. . Before diving into cross-entropy loss and its application to classification, the concept of entropy and cross-entropy must be clear, so this article is dedicated to exploring what these 2 terms mean. . The content and images in this post are inspired by the amazing tutorial A Short Introduction to Entropy, Cross-Entropy and KL-Divergence by Aurélien Géron . Entropy . The concept of entropy and cross-entropy comes from Claude Shannon’s Information theory which he introduces through his classical paper “A Mathematical Theory of Communication” . According to Shannon, Entropy is the minimum no of useful bits required to transfer information from a sender to a receiver. . Let’s understand the two terms by looking into an example. . Suppose that we need to share the weather information of a place with another friend who stays in a different city, and the weather has a 50–50 chance of being sunny or rainy every day. . . This information can be transmitted using just a single bit (0 or 1) and the uncertainty associated with this event is 2 as there are 2 possibilities, either weather is sunny or rainy. . If the probability of occurrence of an event is ppp, then the uncertainty raised due to that event is given as 1p frac{1}{p}p1​ . In our example, the probability of occurrence of both events is 0.5 so the uncertainty for each event is 10.5=2 frac{1}{0.5} = 20.51​=2 . Even if the information is transferred as a string “RAINY” having 5 characters each of 1 byte, the total information transferred is 40 bits but only 1 bit of useful information is transferred. . Given the uncertainty due to an event is NNN, the minimum number of bits required to transfer the information about that event can be calculated as log(N)log(N)log(N) . Here, as uncertainty for weather being rainy or sunny is 2, the minimum no. of useful bits required to transfer information about being sunny or rainy is log(2)=1log(2) = 1log(2)=1 . Note : In the article, log(x)log(x)log(x) means logarithm with base 222 and ln(x)ln(x)ln(x) means natural logarithm with base eee . Now suppose that the event “Weather” had 8 possibilities, all equally likely with 12.5%12.5 %12.5% probability of occurrence of each. . . So now as the no. of uncertainties is 8, the minimum no. of useful bits required to transfer information about each event can be calculated as log(8)=3log(8) = 3log(8)=3 . Let us consider a case which is similar to the 1st case that we saw with 2 possibilities, sunny or rainy, but now both are not equally likely. One occurs with a probability of 75%75 %75% and the other with a probability of 25%25 %25%. . . Now the events are not occuring with equal probabilities, so the uncertainties for the events will be different. the uncertainty of the weather being rainy is 10.25=4 frac{1}{0.25} = 40.251​=4 and for the weather being sunny is 10.75=1.33 frac{1}{0.75} = 1.330.751​=1.33 . The minimum number of useful bits required the information is rainy is log(4)=2log(4) = 2log(4)=2 and for the weather being sunny is log(1.33)=0.4log(1.33) = 0.4log(1.33)=0.4 . This also be derived from the probability directly as, given the probability of a given event is ppp , then the uncertainty associated with the occurrence of that event is 1/p1/p1/p and hence the minimum number of useful bits required to transfer information about it is, . log(1p) or−log(p), since [log(1p)=−log(p)]log left( frac{1}{p} right) text{ or} -log(p), text{ since } [log left( frac{1}{p} right) = - log(p)]log(p1​) or−log(p), since [log(p1​)=−log(p)] . 2 bits are required to say whether the weather is rainy and 0.4 bits are required to say if the weather is sunny, so the average no. of useful bits required to transmit the information can be calculated as, . 0.75×log(10.75)+0.25×log(10.25)=0.810.75 times log left( frac{1}{0.75} right) + 0.25 times log left( frac{1}{0.25} right) = 0.810.75×log(0.751​)+0.25×log(0.251​)=0.81 . So on average, we would receive 0.81 bits of information and this is the minimum number of bits required to transfer the weather information, following the above-mentioned probability distribution.This is known as Entropy. . Entropy:H(p)=−∑n=1npi×log(pi) boxed{Entropy : H(p) = - sum_{n=1}^{n}{p_i times log(p_i)}}Entropy:H(p)=−n=1∑n​pi​×log(pi​)​ . Entropy (expressed in ‘bits’) is a measure of how unpredictable the probability distribution is. So more the individual events vary, the more is its entropy. . Cross-Entropy . Cross entropy is the average message length that is used to transmit the message. . . In this example, there are 8 variations all equally likely. So the entropy of this system is 3, but suppose that the probability distribution changes with probabilities something like this : . . Though the probability distribution has changed, we still use 3 bits to transfer this information. . Now the entropy of this distribution will be, . E=−0.35×log(0.35)+0.35×log(0.35)+0.1×log(0.1)+0.1×log(0.1)+0.04×log(0.04)+0.04×log(0.04)+0.01×log(0.01)+0.01×log(0.01)=2.23 bitsE = -{ 0.35 times log(0.35) + 0.35 times log(0.35)+ 0.1 times log(0.1) + 0.1 times log(0.1) + 0.04 times log(0.04) + 0.04 times log(0.04) + 0.01 times log(0.01) + 0.01 times log(0.01)} = 2.23 text{ bits}E=−0.35×log(0.35)+0.35×log(0.35)+0.1×log(0.1)+0.1×log(0.1)+0.04×log(0.04)+0.04×log(0.04)+0.01×log(0.01)+0.01×log(0.01)=2.23 bits which is the minimum number of useful bits transmitted, and entropy of the system. . So though we are sending 3 bits of information, the user gets 2.23 useful bits. This can be improved by changing the no. of bits used to address each kind of information. Suppose we use a following distribution : . . The average no. of bits transmitted using the following bit pattern is, . CE=0.35×2+0.35×2+0.1×3+0.1×3+0.04×4+0.04×4+0.01×5+0.01×5=2.42 bitsCE = 0.35 times 2 + 0.35 times 2 + 0.1 times 3 + 0.1 times 3 + 0.04 times 4 + 0.04 times 4 + 0.01 times 5 + 0.01 times 5 = 2.42 text{ bits}CE=0.35×2+0.35×2+0.1×3+0.1×3+0.04×4+0.04×4+0.01×5+0.01×5=2.42 bits which is close to the entropy. This is the Cross Entropy . But suppose the same bit pattern is used for a different probability distribution : . . CE=0.01×2+0.01×2+0.04×3+0.04×3+0.1×4+0.1×4+0.35×5+0.35×5=4.58 bitsCE = 0.01 times 2 + 0.01 times 2 + 0.04 times 3 + 0.04 times 3 + 0.1 times 4 + 0.1 times 4 + 0.35 times 5 + 0.35 times 5 = 4.58 text{ bits}CE=0.01×2+0.01×2+0.04×3+0.04×3+0.1×4+0.1×4+0.35×5+0.35×5=4.58 bits which is significantly grater than the entropy. . This happens because the bit code we are using is making some implicit estimation of the probability distribution of the weather as, . p=(12no. of bits) boxed{p = left( frac{1}{2^{ text{no. of bits}}} right)}p=(2no. of bits1​)​ . . So we can express cross-entropy as a function of both the true distribution and predicted distribution as, . Cross Entropy :H(p,q)=−∑n=1npi×log(qi) boxed{ text{Cross Entropy }: H(p,q) = - sum_{n=1}^{n}{p_i times log(q_i)}}Cross Entropy :H(p,q)=−n=1∑n​pi​×log(qi​)​ . Here instead of taking the logloglog of the true probability, we are taking the logloglog of the predicted probability distribution qqq. . Basically, when we know the probability of occurrence of the events, but we don’t know the bit distribution of the events, so a random distribution can be taken and given the probabilities and assumed bit distribution, cross-entropy of the events can be calculated and cross-checked with the original entropy to see if the assumed distribution gives the minimum uncertainty for the given probabilities or not. Hence it is termed as “Cross” entropy. . Usually Cross entropy is larger than the entropy of a distribution. When the predicted distribution is equal to true distribution, the cross-entropy is equal to entropy. . Kullback–Leibler Divergence . The amount by which the cross-entropy exceeds the entropy is called Relative Entropy or commonly known as Kullback-Leibler Divergence or KL Divergence. . dKL(p ∣∣ q)=H(p,q)−H(p) boxed{d_{KL}(p text{ }|| text{ }q) = H(p,q) - H(p)}dKL​(p ∣∣ q)=H(p,q)−H(p)​ . So the key take away from this article is, . given a probability distribution, the minimum average no. of useful bits required to transfer the information about the distribution is its Entropy which can also be said as the minimum possible randomness that can be associated with a probability distribution. . In the last example, we took an assumed bit distribution for each event and found the cross-entropy of that distribution with the original probabilities of the events. This cross-entropy resulted to be higher than the original entropy. So we tried to change the assumed bit distribution so that we can reduce the cross-entropy and make it as close as possible to the entropy. . But wait for a second !! . Isn’t that exactly what we try to do in a classification? . We start with a randomly initialized model that outputs an assumed bit distribution for the different classes that we want to classify, and in the process of training, we try to achieve an optimal distribution that can get us close enough to the lowest possible Entropy for the probability distribution. . So does that mean cross-entropy can be used to quantify how bad is the model performing in assuming the distribution? . Can we use KL Divergence as a metric to measure how bad the model is performing? . In the subsequent article, we shall explore the answer to all these questions and understand the intuition behind why Cross-Entropy is an appropriate loss function for our requirement. . Got some doubts/suggestions? . Please feel free to share your suggestions, questions, queries, and doubts through comments below — I will be happy to talk/discuss them all. .",
            "url": "https://saiamrit.github.io/technical-blog/2021/07/15/cross-entropy.html",
            "relUrl": "/2021/07/15/cross-entropy.html",
            "date": " • Jul 15, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Lets Keep Classification Simple !",
            "content": "Whenever it comes to image domain task like classification, segmentation etc, the first thing that comes to the mind is of a conplex model like Convolutional Neural Network which performs a heavily complicated operation over the image to extract meaningful features and then perform the required task using the features. . . Rather than involving a complicated model with a hope that a complex model is bound to perform beat on the data and give us almost perfect performance, its always wise to come-up with a very simple algorithm based on visual observation or simple statistical studies over the given image data to get a descent baseline performance and then build upon it till we reach a saturation of performance. . Note: Many a times, even a simple algorithm can outperform a complex model with millions of parameters on the test data. . Baseline Model . A simple model which you are confident should perform reasonably well. It should be very simple to implement, and very easy to test, so that you can then test each of your improved ideas, and make sure they are always better than your baseline. Without starting with a sensible baseline, it is very difficult to know whether your super-fancy models are actually any good. . Baseline for MNIST data classification . MNIST is the most basic dataset in computer vision. It itself and its varients serve as baseline datasets for testing most of the new approaches that researchers come up in vision. In this article, we shall try to build a very simple decision policy to classify a give test image into one of the digit classes without involving any convolution operation or complicated feature extraction and classifier. . Setup Fastai and Necessary Imports . !pip install -Uqq fastbook import fastbook fastbook.setup_book() from fastai.vision.all import * from fastbook import * matplotlib.rc(&#39;image&#39;, cmap=&#39;Greys&#39;) . This sets up the fastai library and imports all the APIs to be used later. . Loading Dataset . We will download the MNIST dataset provided by Fastai using the untar_data() function and store the path to each digit folder in a list for preparing the training data. . path = untar_data(URLs.MNIST) training_paths = (path/&#39;training&#39;).ls().sorted() test_paths = (path/&#39;testing&#39;).ls().sorted() . Let&#39;s have a look at some sample images from each class to understand the data and design a baseline algorithm. . sample_images = [Image.open(training_paths[i].ls()[0]) for i in range(10)] show_images(sample_images, nrows=1) . Approach for the Baseline . One good approach to creating a baseline is to think of a simple, easy-to-implement model. The simplest thing that comes to the mind on looking at the images is that: . Each number has a characteristic shape and pixel distribution. Every other image of the same number should ideally possess similar characteristics and pixel distribution. | If we can come up with a representative for each class that posseses characteristics of maximum members of the class and then simply compare every unknown test image with each of this representative and classify it to that class, whose representative is most similar to it. | Similarity can be defined in terms of pixel wise difference which takes into account pixel distribution into the similarity metric. | Finally the most basic representative for a class can be its mean image. | Baseline Algorithm . Prepare training data by stacking up all images of a particular digit into one tensor. | Find the mean image for each of these digits. | Loop over all the test image and find its distance from each of the mean image and consider its label to be the one with the minimum distance. | Calculate accuracy of this method. | Preparing the training and testing tensors . training_data = [] for pth in training_paths: training_data.append(torch.stack([tensor(Image.open(o)) for o in pth.ls()]).float()/255) test_data = [] for pth in test_paths: test_data.append(torch.stack([tensor(Image.open(o)) for o in pth.ls()]).float()/255) . Computing the Mean Image for each class . mean_images = [data.mean(0) for data in training_data] show_images(mean_images, nrows=1) . Computing the similarity . Similarity is computed as the L2 distance or the Euclidean distance between each test image and each of the mean image and storing the minimumum distance as its predicted label. . Finally computing the accuracy of this approach. . total = 0 correct = 0 for i in range(10): total += test_data[i].shape[0] predict = tensor([tensor([F.mse_loss(test_data[i][im],mean_images[mean]).sqrt() for mean in range(10)]).argmin() for im in range(test_data[i].shape[0])]) correct += (predict == i).sum() . print(&#39;Accuracy of simple model is: {:.2f} %&#39;.format(correct/total*100)) . Accuracy of simple model is: 82.03 % . Notice that just using mean image and L2 distance, we can acheive a baseline performance of 82.03% on MNIST data. This is probably the most simple classification strategy that could be thought of. So any model that performs worse than this should not be accepted. . It is really interesting to note that such a simple approach performs descent on a image data which is fairly complicated. The conclusion is to always start with simpe ideas and establish a baseline model and then build upon that idea and improve performance from the benchmark performance acheived by the baseline model. .",
            "url": "https://saiamrit.github.io/technical-blog/jupyter/2021/07/07/simple-classification.html",
            "relUrl": "/jupyter/2021/07/07/simple-classification.html",
            "date": " • Jul 7, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Viola Jones Implementation from Scratch",
            "content": "Data available at : https://drive.google.com/drive/folders/1UBGUwLAqbtt03RtVhSMrD-AAOOWV3oLr?usp=sharing . import numpy as np # from PIL import Image import os import cv2 from functools import partial from multiprocessing import Pool . pos_training_path = &#39;../face_data/small_data/face/pos_integral.npy&#39; neg_training_path = &#39;../face_data/small_data/non/neg_integral.npy&#39; . faces_int_img_training = np.load(pos_training_path)[:500] non_faces_int_img_training = np.load(neg_training_path)[:500] faces_int_img_test = np.load(pos_training_path)[800:900] non_faces_int_img_test = np.load(neg_training_path)[800:900] . print(np.shape(faces_int_img_training)) print(np.shape(non_faces_int_img_training)) print(np.shape(faces_int_img_test)) print(np.shape(non_faces_int_img_test)) . (500, 50, 50) (500, 50, 50) (100, 50, 50) (100, 50, 50) . num_classifiers = 20 min_height = 4 max_height = 10 min_width = 4 max_width = 10 . def enum(**enums): return type(&#39;Enum&#39;, (), enums) FeatureType = enum(TWO_VERTICAL=(1, 2), TWO_HORIZONTAL=(2, 1), THREE_HORIZONTAL=(3, 1), THREE_VERTICAL=(1, 3), FOUR=(2, 2)) FeatureTypes = [FeatureType.TWO_VERTICAL, FeatureType.TWO_HORIZONTAL, FeatureType.THREE_VERTICAL, FeatureType.THREE_HORIZONTAL, FeatureType.FOUR] . class HaarLikeFeature(object): &quot;&quot;&quot; Class representing a haar-like feature. &quot;&quot;&quot; def __init__(self, feature_type, position, width, height, threshold, polarity): &quot;&quot;&quot; Creates a new haar-like feature. &quot;&quot;&quot; self.type = feature_type self.top_left = position self.bottom_right = (position[0] + width, position[1] + height) self.width = width self.height = height self.threshold = threshold self.polarity = polarity self.weight = 1 # def __str__(self): # return str(self.__class__) + &quot;: &quot; + str(self.__dict__) def get_score(self, int_img): &quot;&quot;&quot; Get score for given integral image array. &quot;&quot;&quot; score = 0 if self.type == FeatureType.TWO_VERTICAL: first = sum_region(int_img, self.top_left, (self.top_left[0] + self.width, int(self.top_left[1] + self.height / 2))) second = sum_region(int_img, (self.top_left[0], int(self.top_left[1] + self.height / 2)), self.bottom_right) score = first - second elif self.type == FeatureType.TWO_HORIZONTAL: first = sum_region(int_img, self.top_left, (int(self.top_left[0] + self.width / 2), self.top_left[1] + self.height)) second = sum_region(int_img, (int(self.top_left[0] + self.width / 2), self.top_left[1]), self.bottom_right) score = first - second elif self.type == FeatureType.THREE_HORIZONTAL: first = sum_region(int_img, self.top_left, (int(self.top_left[0] + self.width / 3), self.top_left[1] + self.height)) second = sum_region(int_img, (int(self.top_left[0] + self.width / 3), self.top_left[1]), (int(self.top_left[0] + 2 * self.width / 3), self.top_left[1] + self.height)) third = sum_region(int_img, (int(self.top_left[0] + 2 * self.width / 3), self.top_left[1]), self.bottom_right) score = first - second + third elif self.type == FeatureType.THREE_VERTICAL: first = sum_region(int_img, self.top_left, (self.bottom_right[0], int(self.top_left[1] + self.height / 3))) second = sum_region(int_img, (self.top_left[0], int(self.top_left[1] + self.height / 3)), (self.bottom_right[0], int(self.top_left[1] + 2 * self.height / 3))) third = sum_region(int_img, (self.top_left[0], int(self.top_left[1] + 2 * self.height / 3)), self.bottom_right) score = first - second + third elif self.type == FeatureType.FOUR: # top left area first = sum_region(int_img, self.top_left, (int(self.top_left[0] + self.width / 2), int(self.top_left[1] + self.height / 2))) # top right area second = sum_region(int_img, (int(self.top_left[0] + self.width / 2), self.top_left[1]), (self.bottom_right[0], int(self.top_left[1] + self.height / 2))) # bottom left area third = sum_region(int_img, (self.top_left[0], int(self.top_left[1] + self.height / 2)), (int(self.top_left[0] + self.width / 2), self.bottom_right[1])) # bottom right area fourth = sum_region(int_img, (int(self.top_left[0] + self.width / 2), int(self.top_left[1] + self.height / 2)), self.bottom_right) score = first - second - third + fourth return score def get_vote(self, int_img): &quot;&quot;&quot; Get vote of this feature for given integral image. &quot;&quot;&quot; score = self.get_score(int_img) return self.weight * (1 if score &lt; self.polarity * self.threshold else -1) . def create_features(img_height, img_width, min_feature_width, max_feature_width, min_feature_height, max_feature_height): &#39;&#39;&#39; Runs multiple sized windows and creates features within the windows at each location &#39;&#39;&#39; print(&#39;Creating haar-like features..&#39;) features = [] print(FeatureTypes) for feature in FeatureTypes: # FeatureTypes are just tuples feature_start_width = max(min_feature_width, feature[0]) for feature_width in range(feature_start_width, max_feature_width, feature[0]): feature_start_height = max(min_feature_height, feature[1]) for feature_height in range(feature_start_height, max_feature_height, feature[1]): for x in range(img_width - feature_width): for y in range(img_height - feature_height): features.append(HaarLikeFeature(feature, (x, y), feature_width, feature_height, 0, 1)) features.append(HaarLikeFeature(feature, (x, y), feature_width, feature_height, 0, -1)) print(str(len(features)) + &#39; features created. n&#39;) return features def get_feature_vote(feature, image): return feature.get_vote(image) . def sum_region(integral_img_arr, top_left, bottom_right): &quot;&quot;&quot; Calculates the sum in the rectangle specified by the given tuples of features. &quot;&quot;&quot; # swap tuples top_left = (top_left[1], top_left[0]) bottom_right = (bottom_right[1], bottom_right[0]) if top_left == bottom_right: return integral_img_arr[top_left] top_right = (bottom_right[0], top_left[1]) bottom_left = (top_left[0], bottom_right[1]) return integral_img_arr[bottom_right] - integral_img_arr[top_right] - integral_img_arr[bottom_left] + integral_img_arr[top_left] . def learn(pos_ii, neg_ii, num_classifiers=-1, min_width=1, max_width=-1, min_height=1, max_height=-1): &#39;&#39;&#39; the main learn function that creates classifiers, calculates scores at various windows and locations and returns the classifiers &#39;&#39;&#39; num_pos = len(pos_ii) num_neg = len(neg_ii) num_imgs = num_pos + num_neg img_height, img_width = pos_ii[0].shape print(num_pos, num_neg, num_imgs, img_height, img_width) # set maximum feature heights and widths max_height = img_height if max_height == -1 else max_height max_width = img_width if max_width == -1 else max_width # print(max_height, max_width) # Create initial weights and labels pos_weights = np.ones(num_pos) * 1. / (2 * num_pos) neg_weights = np.ones(num_neg) * 1. / (2 * num_neg) weights = np.hstack((pos_weights, neg_weights)) labels = np.hstack((np.ones(num_pos), np.ones(num_neg) * -1)) # print(pos_weights, neg_weights, weights,labels) images = np.vstack((pos_ii, neg_ii)) print(np.shape(images)) features = create_features(img_height, img_width, min_width, max_width, min_height, max_height) num_features = len(features) feature_indexes = list(range(num_features)) num_classifiers = num_features if num_classifiers == -1 else num_classifiers print(&#39;Calculating scores for images..&#39;) votes = np.zeros((num_imgs, num_features)) # print(votes.shape) pool = Pool(processes=None) for i in range(num_imgs): votes[i] = np.array(list(pool.map(partial(get_feature_vote, image=images[i]), features))) # for j in range(num_features): # votes[i] = np.array(get_feature_vote(features[j], images[i])) classifiers = [] print(&#39;Selecting classifiers..&#39;) for t in range(num_classifiers): classification_errors = np.zeros(len(feature_indexes)) # normalize weights weights *= 1. / np.sum(weights) # select best classifier based on the weighted error for f in range(len(feature_indexes)): f_idx = feature_indexes[f] # classifier error is the sum of image weights where the classifier # is right error = sum(map(lambda img_idx: weights[img_idx] if labels[img_idx] != votes[img_idx, f_idx] else 0, range(num_imgs))) classification_errors[f] = error min_error_idx = np.argmin(classification_errors) best_error = classification_errors[min_error_idx] best_feature_idx = feature_indexes[min_error_idx] # set feature weight best_feature = features[best_feature_idx] feature_weight = 0.5 * np.log((1 - best_error) / best_error) best_feature.weight = feature_weight classifiers.append(best_feature) # update image weights weights = np.array(list(map(lambda img_idx: weights[img_idx] * np.sqrt((1-best_error)/best_error) if labels[img_idx] != votes[img_idx, best_feature_idx] else weights[img_idx] * np.sqrt(best_error/(1-best_error)), range(num_imgs)))) # remove feature (a feature can&#39;t be selected twice) feature_indexes.remove(best_feature_idx) return classifiers . classifiers = learn(faces_int_img_training, non_faces_int_img_training, num_classifiers, min_height, max_height, min_width, max_width) . 500 500 1000 50 50 (1000, 50, 50) Creating haar-like features.. [(1, 2), (2, 1), (1, 3), (3, 1), (2, 2)] ..done. 265572 features created. Calculating scores for images.. Selecting classifiers.. . . def ensemble_vote(int_img, classifiers): &quot;&quot;&quot; Classifies given integral image (numpy array) using given classifiers, &quot;&quot;&quot; return 1 if sum([c.get_vote(int_img) for c in classifiers]) &gt;= 0 else 0 def ensemble_vote_all(int_imgs, classifiers): &quot;&quot;&quot; Classifies given list of integral images (numpy arrays) using classifiers, &quot;&quot;&quot; vote_partial = partial(ensemble_vote, classifiers=classifiers) return list(map(vote_partial, int_imgs)) . print(np.shape(faces_int_img_test))Data available at : . (100, 50, 50) . print(&#39;Testing selected classifiers..&#39;) correct_faces = 0 correct_non_faces = 0 correct_faces = sum(ensemble_vote_all(faces_int_img_test, classifiers)) correct_non_faces = len(non_faces_int_img_test) - sum(ensemble_vote_all(non_faces_int_img_test, classifiers)) print(correct_faces, correct_non_faces, len(classifiers)) print(&#39;..done. n nResult: n Faces: &#39; + str(correct_faces) + &#39;/&#39; + str(len(faces_int_img_test)) + &#39; (&#39; + str((floati.e. if the sum of all classifier votes is greater 0, an image is classified positively (1) else negatively (0). The threshold is 0, because votes can be +1 or -1. :param int_imgs: List of integral images to be classified :type int_imgs: list[numpy.ndarray] :param classifiers: List of classifiers :type classifiers: list[violajones.HaarLikeFeature.HaarLikeFeature] :return: List of assigned labels, 1 if image was classified positively, else 0 :rtype: list[int](correct_faces) / len(faces_int_img_test)) * 100) + &#39;%) n non-Faces: &#39; + str(correct_non_faces) + &#39;/&#39; + str(len(non_faces_int_img_test)) + &#39; (&#39; + str((float(correct_non_faces) / len(non_faces_int_img_test)) * 100) + &#39;%)&#39;) . Testing selected classifiers.. 82 70 20 ..done. Result: Faces: 82/100 (82.0%) non-Faces: 70/100 (70.0%) .",
            "url": "https://saiamrit.github.io/technical-blog/2021/03/21/viola.html",
            "relUrl": "/2021/03/21/viola.html",
            "date": " • Mar 21, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Grab Cut from scratch (Image Segmentation using MRFs)",
            "content": "Here we will implement the GrabCut method mentioned in this paper. It is essentially an iterative version of GraphCut as shown in the figure below. . The code below takes an input image and follows these steps: . It requires a bounding box to be drawn by the user to roughly segment out the foreground pixels | It runs an initial min-cut optimization using the provided annotation | The result of this optimization gives an initial segmentation | To further refine this segmentation, the user provides two kinds of strokes to aid the optimization strokes on the background pixels | strokes on the foreground pixels | . | The algorithm now utilizes this to refine the original segmentation | . You can view this video to get a better idea of the steps involved. . Image segmentation is one exciting application of MRFs. You can further read about other applications of MRFs for Computer Vision here. . Useful Links . https://courses.engr.illinois.edu/cs543/sp2011/lectures/Lecture%2012%20-%20MRFs%20and%20Graph%20Cut%20Segmentation%20-%20Vision_Spring2011.pdf | . import sys import os import numpy as np import math import cv2 as cv import igraph as ig from sklearn.cluster import KMeans . def score_formula(mult,mat): score = np.exp(-.5 * mult) / np.sqrt(2 * np.pi)/np.sqrt(np.linalg.det(mat)) return score class GaussianMixture: def __init__(self, X, gmm_components): self.n_components = gmm_components self.n_features = X.shape[1] self.n_samples = np.zeros(self.n_components) self.coefs = np.zeros(self.n_components) self.means = np.zeros((self.n_components, self.n_features)) self.covariances = np.zeros( (self.n_components, self.n_features, self.n_features)) self.init_with_kmeans(X) def init_with_kmeans(self, X): label = KMeans(n_clusters=self.n_components, n_init=1).fit(X).labels_ self.fit(X, label) def calc_score(self, X, ci): score = np.zeros(X.shape[0]) if self.coefs[ci] &gt; 0: diff = X - self.means[ci] Tdiff = diff.T inv_cov = np.linalg.inv(self.covariances[ci]) dot = np.dot(inv_cov, Tdiff) Tdot = dot.T mult = np.einsum(&#39;ij,ij-&gt;i&#39;, diff, Tdot) score = score_formula(mult,self.covariances[ci]) return score def calc_prob(self, X): prob = [] for ci in range(self.n_components): score = np.zeros(X.shape[0]) if self.coefs[ci] &gt; 0: diff = X - self.means[ci] Tdiff = diff.T inv_cov = np.linalg.inv(self.covariances[ci]) dot = np.dot(inv_cov, Tdiff) Tdot = dot.T mult = np.einsum(&#39;ij,ij-&gt;i&#39;, diff, Tdot) score = score_formula(mult,self.covariances[ci]) prob.append(score) ans = np.dot(self.coefs, prob) return ans def which_component(self, X): prob = [] for ci in range(self.n_components): score = self.calc_score(X,ci) prob.append(score) prob = np.array(prob).T return np.argmax(prob, axis=1) def fit(self, X, labels): assert self.n_features == X.shape[1] self.n_samples[:] = 0 self.coefs[:] = 0 uni_labels, count = np.unique(labels, return_counts=True) self.n_samples[uni_labels] = count variance = 0.01 for ci in uni_labels: n = self.n_samples[ci] sum = np.sum(self.n_samples) self.coefs[ci] = n / sum self.means[ci] = np.mean(X[ci == labels], axis=0) if self.n_samples[ci] &lt;= 1: self.covariances[ci] = 0 else: self.covariances[ci] = np.cov(X[ci == labels].T) det = np.linalg.det(self.covariances[ci]) if det &lt;= 0: self.covariances[ci] += np.eye(self.n_features) * variance det = np.linalg.det(self.covariances[ci]) . def construct_gc_graph(img,mask,gc_source,gc_sink,fgd_gmm,bgd_gmm,gamma,rows,cols,left_V, up_V, neighbours, upleft_V=None,upright_V=None): bgd_indexes = np.where(mask.reshape(-1) == DRAW_BG[&#39;val&#39;]) fgd_indexes = np.where(mask.reshape(-1) == DRAW_FG[&#39;val&#39;]) pr_indexes = np.where(np.logical_or(mask.reshape(-1) == DRAW_PR_BG[&#39;val&#39;],mask.reshape(-1) == DRAW_PR_FG[&#39;val&#39;])) # print(&#39;bgd count: %d, fgd count: %d, uncertain count: %d&#39; % (len(bgd_indexes[0]), len(fgd_indexes[0]), len(pr_indexes[0]))) edges = [] gc_graph_capacity = [] edges.extend(list(zip([gc_source] * pr_indexes[0].size, pr_indexes[0]))) _D = -np.log(bgd_gmm.calc_prob(img.reshape(-1, 3)[pr_indexes])) gc_graph_capacity.extend(_D.tolist()) edges.extend(list(zip([gc_sink] * pr_indexes[0].size, pr_indexes[0]))) _D = -np.log(fgd_gmm.calc_prob(img.reshape(-1, 3)[pr_indexes])) gc_graph_capacity.extend(_D.tolist()) edges.extend(list(zip([gc_source] * bgd_indexes[0].size, bgd_indexes[0]))) gc_graph_capacity.extend([0] * bgd_indexes[0].size) edges.extend(list(zip([gc_sink] * bgd_indexes[0].size, bgd_indexes[0]))) gc_graph_capacity.extend([9 * gamma] * bgd_indexes[0].size) edges.extend(list(zip([gc_source] * fgd_indexes[0].size, fgd_indexes[0]))) gc_graph_capacity.extend([9 * gamma] * fgd_indexes[0].size) edges.extend(list(zip([gc_sink] * fgd_indexes[0].size, fgd_indexes[0]))) gc_graph_capacity.extend([0] * fgd_indexes[0].size) img_indexes = np.arange(rows*cols,dtype=np.uint32).reshape(rows,cols) temp1 = img_indexes[:, 1:] temp2 = img_indexes[:, :-1] mask1 = temp1.reshape(-1) mask2 = temp2.reshape(-1) edges.extend(list(zip(mask1, mask2))) gc_graph_capacity.extend(left_V.reshape(-1).tolist()) temp1 = img_indexes[1:, 1:] temp2 = img_indexes[:-1, :-1] mask1 = temp1.reshape(-1) mask2 = temp2.reshape(-1) edges.extend(list(zip(mask1, mask2))) gc_graph_capacity.extend(up_V.reshape(-1).tolist()) if neighbours == 8: temp1 = img_indexes[1:, :] temp2 = img_indexes[:-1, :] mask1 = temp1.reshape(-1) mask2 = temp2.reshape(-1) edges.extend(list(zip(mask1, mask2))) gc_graph_capacity.extend(upleft_V.reshape(-1).tolist()) temp1 = img_indexes[1:, :-1] temp2 = img_indexes[:-1, 1:] mask1 = temp1.reshape(-1) mask2 = temp2.reshape(-1) edges.extend(list(zip(mask1, mask2))) gc_graph_capacity.extend(upright_V.reshape(-1).tolist()) gc_graph = ig.Graph(cols * rows + 2) gc_graph.add_edges(edges) return gc_graph,gc_source,gc_sink,gc_graph_capacity def estimate_segmentation(mask,gc_graph,gc_source,gc_sink,gc_graph_capacity,rows,cols): mincut = gc_graph.st_mincut(gc_source,gc_sink, gc_graph_capacity) # print(&#39;foreground pixels: %d, background pixels: %d&#39; % (len(mincut.partition[0]), len(mincut.partition[1]))) pr_indexes = np.where(np.logical_or(mask == DRAW_PR_BG[&#39;val&#39;], mask == DRAW_PR_FG[&#39;val&#39;])) img_indexes = np.arange(rows * cols,dtype=np.uint32).reshape(rows, cols) mask[pr_indexes] = np.where(np.isin(img_indexes[pr_indexes], mincut.partition[0]),DRAW_PR_FG[&#39;val&#39;], DRAW_PR_BG[&#39;val&#39;]) bgd_indexes = np.where(np.logical_or(mask == DRAW_BG[&#39;val&#39;],mask == DRAW_PR_BG[&#39;val&#39;])) fgd_indexes = np.where(np.logical_or(mask == DRAW_FG[&#39;val&#39;],mask == DRAW_PR_FG[&#39;val&#39;])) # print(&#39;probble background count: %d, probable foreground count: %d&#39; % (bgd_indexes[0].size,fgd_indexes[0].size)) return pr_indexes,img_indexes,mask,bgd_indexes,fgd_indexes def classify_pixels(mask): bgd_indexes = np.where(np.logical_or(mask == DRAW_BG[&#39;val&#39;], mask == DRAW_PR_BG[&#39;val&#39;])) fgd_indexes = np.where(np.logical_or(mask == DRAW_FG[&#39;val&#39;], mask == DRAW_PR_FG[&#39;val&#39;])) return fgd_indexes, bgd_indexes def compute_smoothness(img, rows, cols, neighbours): left_diff = img[:, 1:] - img[:, :-1] up_diff = img[1:, :] - img[:-1, :] sq_left_diff = np.square(left_diff) sq_up_diff = np.square(up_diff) beta_sum = (np.sum(sq_left_diff) + np.sum(sq_up_diff)) avg = (2 * rows * cols) - cols - rows if neighbours == 8: upleft_diff = img[1:, 1:] - img[:-1, :-1] upright_diff = img[1:, :-1] - img[:-1, 1:] sq_upleft_diff = np.square(upleft_diff) sq_upright_diff = np.square(upright_diff) beta_sum += np.sum(sq_upleft_diff) + np.sum(sq_upright_diff) avg += (2 * rows * cols) - (2 * cols) - (2 * rows) + 2 beta = avg / (2 * beta_sum) # print(&#39;Beta:&#39;,beta) left_V = gamma * np.exp(-beta * np.sum(np.square(left_diff), axis=2)) up_V = gamma * np.exp(-beta * np.sum(np.square(up_diff), axis=2)) if neighbours == 8: upleft_V = gamma / np.sqrt(2) * np.exp(-beta * np.sum(np.square(upleft_diff), axis=2)) upright_V = gamma / np.sqrt(2) * np.exp(-beta * np.sum(np.square(upright_diff), axis=2)) return gamma, left_V, up_V, upleft_V, upright_V else: return gamma, left_V, up_V, None, None def initialize_gmm(img, bgd_indexes, fgd_indexes, gmm_components): bgd_gmm = GaussianMixture(img[bgd_indexes], gmm_components) fgd_gmm = GaussianMixture(img[fgd_indexes], gmm_components) return fgd_gmm, bgd_gmm def GrabCut(img, mask, rect, gmm_components, gamma, neighbours, n_iters): img = np.asarray(img, dtype=np.float64) rows,cols, _ = img.shape if rect is not None: mask[rect[1]:rect[1] + rect[3],rect[0]:rect[0] + rect[2]] = DRAW_PR_FG[&#39;val&#39;] fgd_indexes, bgd_indexes = classify_pixels(mask) gmm_components = gmm_components gamma = gamma beta = 0 neighbours = neighbours left_V = np.empty((rows,cols - 1)) up_V = np.empty((rows - 1,cols)) if neighbours == 8: upleft_V = np.empty((rows - 1,cols - 1)) upright_V = np.empty((rows - 1,cols - 1)) bgd_gmm = None fgd_gmm = None comp_idxs = np.empty((rows,cols), dtype=np.uint32) gc_graph = None gc_graph_capacity = None gc_source = cols*rows gc_sink = gc_source + 1 gamma, left_V, up_V, upleft_V, upright_V = compute_smoothness(img, rows, cols, neighbours) fwd_gmm, bgd_gmm = initialize_gmm(img, bgd_indexes, fgd_indexes, gmm_components) n_iters = n_iters for iters in range(n_iters): fgd_gmm, bgd_gmm = initialize_gmm(img, bgd_indexes, fgd_indexes, gmm_components) if neighbours == 8: gc_graph,gc_source,gc_sink,gc_graph_capacity = construct_gc_graph(img,mask,gc_source,gc_sink, fgd_gmm,bgd_gmm,gamma,rows, cols,left_V, up_V, neighbours, upleft_V, upright_V) else: gc_graph,gc_source,gc_sink,gc_graph_capacity = construct_gc_graph(img,mask,gc_source,gc_sink, fgd_gmm,bgd_gmm,gamma,rows, cols,left_V, up_V, neighbours, upleft_V=None, upright_V=None) pr_indexes,img_indexes,mask,bgd_indexes,fgd_indexes = estimate_segmentation(mask,gc_graph,gc_source, gc_sink,gc_graph_capacity, rows,cols) return mask . class EventHandler: &quot;&quot;&quot; Class for handling user input during segmentation iterations &quot;&quot;&quot; def __init__(self, flags, img, _mask, colors): self.FLAGS = flags self.ix = -1 self.iy = -1 self.img = img self.img2 = self.img.copy() self._mask = _mask self.COLORS = colors @property def image(self): return self.img @image.setter def image(self, img): self.img = img @property def mask(self): return self._mask @mask.setter def mask(self, _mask): self._mask = _mask @property def flags(self): return self.FLAGS @flags.setter def flags(self, flags): self.FLAGS = flags def handler(self, event, x, y, flags, param): # Draw the rectangle first if event == cv.EVENT_RBUTTONDOWN: self.FLAGS[&#39;DRAW_RECT&#39;] = True self.ix, self.iy = x,y elif event == cv.EVENT_MOUSEMOVE: if self.FLAGS[&#39;DRAW_RECT&#39;] == True: self.img = self.img2.copy() cv.rectangle(self.img, (self.ix, self.iy), (x, y), self.COLORS[&#39;BLUE&#39;], 2) cv.rectangle(self._mask, (self.ix, self.iy), (x, y), self.FLAGS[&#39;value&#39;][&#39;val&#39;], -1) self.FLAGS[&#39;RECT&#39;] = (min(self.ix, x), min(self.iy, y), abs(self.ix - x), abs(self.iy - y)) self.FLAGS[&#39;rect_or_mask&#39;] = 0 elif event == cv.EVENT_RBUTTONUP: self.FLAGS[&#39;DRAW_RECT&#39;] = False self.FLAGS[&#39;rect_over&#39;] = True cv.rectangle(self.img, (self.ix, self.iy), (x, y), self.COLORS[&#39;BLUE&#39;], 2) cv.rectangle(self._mask, (self.ix, self.iy), (x, y), self.FLAGS[&#39;value&#39;][&#39;val&#39;], -1) self.FLAGS[&#39;RECT&#39;] = (min(self.ix, x), min(self.iy, y), abs(self.ix - x), abs(self.iy - y)) self.FLAGS[&#39;rect_or_mask&#39;] = 0 # Draw strokes for refinement if event == cv.EVENT_LBUTTONDOWN: if self.FLAGS[&#39;rect_over&#39;] == False: print(&#39;Draw the rectangle first.&#39;) else: self.FLAGS[&#39;DRAW_STROKE&#39;] = True cv.circle(self.img, (x,y), 3, self.FLAGS[&#39;value&#39;][&#39;color&#39;], -1) cv.circle(self._mask, (x,y), 3, self.FLAGS[&#39;value&#39;][&#39;val&#39;], -1) elif event == cv.EVENT_MOUSEMOVE: if self.FLAGS[&#39;DRAW_STROKE&#39;] == True: cv.circle(self.img, (x, y), 3, self.FLAGS[&#39;value&#39;][&#39;color&#39;], -1) cv.circle(self._mask, (x, y), 3, self.FLAGS[&#39;value&#39;][&#39;val&#39;], -1) elif event == cv.EVENT_LBUTTONUP: if self.FLAGS[&#39;DRAW_STROKE&#39;] == True: self.FLAGS[&#39;DRAW_STROKE&#39;] = False cv.circle(self.img, (x, y), 3, self.FLAGS[&#39;value&#39;][&#39;color&#39;], -1) cv.circle(self._mask, (x, y), 3, self.FLAGS[&#39;value&#39;][&#39;val&#39;], -1) . def load_image(filename, color_space=&#39;RGB&#39;, scale=1.0): im = cv.imread(filename) if color_space == &quot;RGB&quot;: pass # im = cv.cvtColor(im, cv.COLOR_BGR2RGB) elif color_space == &quot;HSV&quot;: im = cv.cvtColor(im, cv.COLOR_BGR2HSV) elif color_space == &quot;LAB&quot;: im = cv.cvtColor(im, cv.COLOR_BGR2LAB) if not scale == 1.0: im = cv.resize(im, (int(im.shape[1]*scale), int(im.shape[0]*scale))) return im . COLORS = { &#39;BLACK&#39; : [0,0,0], &#39;RED&#39; : [0, 0, 255], &#39;GREEN&#39; : [0, 255, 0], &#39;BLUE&#39; : [255, 0, 0], &#39;WHITE&#39; : [255,255,255] } gmm_components = 15 gamma = 30 neighbours = 8 color_space = &#39;RGB&#39; n_iters = 5 DRAW_PR_BG = {&#39;color&#39; : COLORS[&#39;BLACK&#39;], &#39;val&#39; : 2} DRAW_PR_FG = {&#39;color&#39; : COLORS[&#39;WHITE&#39;], &#39;val&#39; : 3} DRAW_BG = {&#39;color&#39; : COLORS[&#39;BLACK&#39;], &#39;val&#39; : 0} DRAW_FG = {&#39;color&#39; : COLORS[&#39;WHITE&#39;], &#39;val&#39; : 1} . def run(filename, gamma=50, gmm_components=7, neighbours=8, color_space=&#39;RGB&#39;): &quot;&quot;&quot; Main loop that implements GrabCut. Input -- filename (str) : Path to image &quot;&quot;&quot; COLORS = { &#39;BLACK&#39; : [0,0,0], &#39;RED&#39; : [0, 0, 255], &#39;GREEN&#39; : [0, 255, 0], &#39;BLUE&#39; : [255, 0, 0], &#39;WHITE&#39; : [255,255,255] } # gmm_components = 4 # gamma = 30 # neighbours = 8 # color_space = &#39;RGB&#39; DRAW_PR_BG = {&#39;color&#39; : COLORS[&#39;BLACK&#39;], &#39;val&#39; : 2} DRAW_PR_FG = {&#39;color&#39; : COLORS[&#39;WHITE&#39;], &#39;val&#39; : 3} DRAW_BG = {&#39;color&#39; : COLORS[&#39;BLACK&#39;], &#39;val&#39; : 0} DRAW_FG = {&#39;color&#39; : COLORS[&#39;WHITE&#39;], &#39;val&#39; : 1} FLAGS = { &#39;RECT&#39; : (0, 0, 1, 1), &#39;DRAW_STROKE&#39;: False, # flag for drawing strokes &#39;DRAW_RECT&#39; : False, # flag for drawing rectangle &#39;rect_over&#39; : False, # flag to check if rectangle is drawn &#39;rect_or_mask&#39; : -1, # flag for selecting rectangle or stroke mode &#39;value&#39; : DRAW_PR_FG, # drawing strokes initialized to mark foreground } img = load_image(filename, color_space, scale=0.75) img2 = img.copy() mask = np.ones(img.shape[:2], dtype = np.uint8) * DRAW_PR_BG[&#39;val&#39;] # mask is a binary array with : 0 - background pixels # 1 - foreground pixels output = np.zeros(img.shape, np.uint8) # output image to be shown # Input and segmentation windows cv.namedWindow(&#39;Input Image&#39;) cv.namedWindow(&#39;Segmented image&#39;) EventObj = EventHandler(FLAGS, img, mask, COLORS) cv.setMouseCallback(&#39;Input Image&#39;, EventObj.handler) cv.moveWindow(&#39;Input Image&#39;, img.shape[1] + 10, 90) while(1): img = EventObj.image mask = EventObj.mask FLAGS = EventObj.flags cv.imshow(&#39;Segmented image&#39;, output) cv.imshow(&#39;Input Image&#39;, img) k = cv.waitKey(1) # key bindings if k == 27: # esc to exit break elif k == ord(&#39;0&#39;): # Strokes for background FLAGS[&#39;value&#39;] = DRAW_BG elif k == ord(&#39;1&#39;): # FG drawing FLAGS[&#39;value&#39;] = DRAW_FG elif k == ord(&#39;r&#39;): # reset everything FLAGS[&#39;RECT&#39;] = (0, 0, 1, 1) FLAGS[&#39;DRAW_STROKE&#39;] = False FLAGS[&#39;DRAW_RECT&#39;] = False FLAGS[&#39;rect_or_mask&#39;] = -1 FLAGS[&#39;rect_over&#39;] = False FLAGS[&#39;value&#39;] = DRAW_PR_FG img = img2.copy() mask = np.zeros(img.shape[:2], dtype = np.uint8) EventObj.image = img EventObj.mask = mask output = np.zeros(img.shape, np.uint8) elif k == 13: # Press carriage return to initiate segmentation #-# # Implement GrabCut here. # # Function should return a mask which can be used # # to segment the original image as shown on L90 # #-# rect = FLAGS[&#39;RECT&#39;] mask = GrabCut(img2,mask,rect, gmm_components, gamma, neighbours, n_iters) EventObj.flags = FLAGS mask2 = np.where((mask == 1) + (mask == 3), 255, 0).astype(&#39;uint8&#39;) output = cv.bitwise_and(img2, img2, mask=mask2) . if __name__ == &#39;__main__&#39;: filename = &#39;../images/sheep.jpg&#39; # Path to image file run(filename, gamma, gmm_components, neighbours, color_space) cv.destroyAllWindows() . Report . 1. Different Color Spaces . RGB Space . HSV Space . LAB Space . Conclusion - The results are better on the RGB color space . 2. Different size of bounding box . Tight Bounding Box . Large Bounding Box . Conclusion - The results are better with a tighter bounding box . 3. Different value of Gamma . Gamma = 10 . Gamma = 50 . Gamma = 100 . Conclusion - The results are better with a gamma around 50. With lower gamma, lot of area is segmented out and with a higher gamma, some foreground areas are missed out. . 4. 4 or 8 Connectivity . 4 Connectivity . 8 Connectivity . Conclusion - The results are better with 8 connectivity, because similarity is more appropriately computed by taking into account more nearby pixels. . 5. Number of GMM Components . 2 Components . 5 Components . 10 Components . Conclusion - The results are better with a higher no of components but with increasing no. of components, the results donot change significantly. .",
            "url": "https://saiamrit.github.io/technical-blog/2021/02/13/grab.html",
            "relUrl": "/2021/02/13/grab.html",
            "date": " • Feb 13, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Vanilla GAN",
            "content": "%load_ext autoreload %matplotlib inline . %autoreload 2 from IPython import display from utils import Logger import torch from torch import nn, optim from torch.autograd.variable import Variable from torchvision import transforms, datasets . DATA_FOLDER = &#39;./torch_data/VGAN/MNIST&#39; . def mnist_data(): compose = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((.5,), (.5,)) ]) out_dir = &#39;{}/dataset&#39;.format(DATA_FOLDER) return datasets.MNIST(root=out_dir, train=True, transform=compose, download=True) . data = mnist_data() # Create loader with data, so that we can iterate over it data_loader = torch.utils.data.DataLoader(data, batch_size=100, shuffle=True) # Num batches num_batches = len(data_loader) . Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./torch_data/VGAN/MNIST/dataset/MNIST/raw/train-images-idx3-ubyte.gz . 100.1% . Extracting ./torch_data/VGAN/MNIST/dataset/MNIST/raw/train-images-idx3-ubyte.gz to ./torch_data/VGAN/MNIST/dataset/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./torch_data/VGAN/MNIST/dataset/MNIST/raw/train-labels-idx1-ubyte.gz . 113.5% . Extracting ./torch_data/VGAN/MNIST/dataset/MNIST/raw/train-labels-idx1-ubyte.gz to ./torch_data/VGAN/MNIST/dataset/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./torch_data/VGAN/MNIST/dataset/MNIST/raw/t10k-images-idx3-ubyte.gz . 100.4% . Extracting ./torch_data/VGAN/MNIST/dataset/MNIST/raw/t10k-images-idx3-ubyte.gz to ./torch_data/VGAN/MNIST/dataset/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./torch_data/VGAN/MNIST/dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz . 180.4%/opt/conda/conda-bld/pytorch_1587428207430/work/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. . Extracting ./torch_data/VGAN/MNIST/dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./torch_data/VGAN/MNIST/dataset/MNIST/raw Processing... Done! . class Discriminator(nn.Module): def __init__(self): super(Discriminator, self).__init__() n_input = 784 n_out = 1 self.layer0 = nn.Sequential( nn.Linear(n_input, 1024), nn.ReLU(), nn.Dropout(0.4) ) self.layer1 = nn.Sequential( nn.Linear(1024, 512), nn.ReLU(), nn.Dropout(0.4) ) self.layer2 = nn.Sequential( nn.Linear(512, 256), nn.ReLU(), nn.Dropout(0.4) ) self.layer3 = nn.Sequential( torch.nn.Linear(256, n_out), torch.nn.Sigmoid() ) def forward(self, x): x = self.layer0(x) x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) return x def image_to_vector(image): return image.view(image.size(0), 784) def vector_to_image(vector): return vector.view(vector.size(0), 1, 28, 28) . class Generator(nn.Module): def __init__(self): super(Generator, self).__init__() n_inputs = 100 n_out = 784 self.layer0 = nn.Sequential( nn.Linear(n_inputs, 256), nn.ReLU() ) self.layer1 = nn.Sequential( nn.Linear(256, 512), nn.ReLU() ) self.layer2 = nn.Sequential( nn.Linear(512, 1024), nn.ReLU() ) self.layer3 = nn.Sequential( nn.Linear(1024, n_out), nn.Tanh() ) def forward(self, x): x = self.layer0(x) x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) return x def noise(size): n = Variable(torch.randn(size,100)) if torch.cuda.is_available(): return n.cuda() return n . discriminator = Discriminator() generator = Generator() if torch.cuda.is_available(): discriminator.cuda() generator.cuda() . print(discriminator) print(generator) . Discriminator( (layer0): Sequential( (0): Linear(in_features=784, out_features=1024, bias=True) (1): ReLU() (2): Dropout(p=0.4, inplace=False) ) (layer1): Sequential( (0): Linear(in_features=1024, out_features=512, bias=True) (1): ReLU() (2): Dropout(p=0.4, inplace=False) ) (layer2): Sequential( (0): Linear(in_features=512, out_features=256, bias=True) (1): ReLU() (2): Dropout(p=0.4, inplace=False) ) (layer3): Sequential( (0): Linear(in_features=256, out_features=1, bias=True) (1): Sigmoid() ) ) Generator( (layer0): Sequential( (0): Linear(in_features=100, out_features=256, bias=True) (1): ReLU() ) (layer1): Sequential( (0): Linear(in_features=256, out_features=512, bias=True) (1): ReLU() ) (layer2): Sequential( (0): Linear(in_features=512, out_features=1024, bias=True) (1): ReLU() ) (layer3): Sequential( (0): Linear(in_features=1024, out_features=784, bias=True) (1): Tanh() ) ) . d_optimizer = optim.Adam(discriminator.parameters(), lr = 0.0001) g_optimizer = optim.Adam(generator.parameters(), lr = 0.0001) loss = nn.BCELoss() d_steps = 1 epochs = 200 . def real_data_target(size): &#39;&#39;&#39; Tensor containing ones, with shape = size &#39;&#39;&#39; data = Variable(torch.ones(size, 1)) if torch.cuda.is_available(): return data.cuda() return data def fake_data_target(size): &#39;&#39;&#39; Tensor containing zeros, with shape = size &#39;&#39;&#39; data = Variable(torch.zeros(size, 1)) if torch.cuda.is_available(): return data.cuda() return data . def train_discriminator(optimizer, real_data, fake_data): # Reset gradients optimizer.zero_grad() # 1.1 Train on Real Data prediction_real = discriminator(real_data) # Calculate error and backpropagate error_real = loss(prediction_real, real_data_target(real_data.size(0))) error_real.backward() # 1.2 Train on Fake Data prediction_fake = discriminator(fake_data) # Calculate error and backpropagate error_fake = loss(prediction_fake, fake_data_target(real_data.size(0))) error_fake.backward() # 1.3 Update weights with gradients optimizer.step() # Return error return error_real + error_fake, prediction_real, prediction_fake def train_generator(optimizer, fake_data): # 2. Train Generator # Reset gradients optimizer.zero_grad() # Sample noise and generate fake data prediction = discriminator(fake_data) # Calculate error and backpropagate error = loss(prediction, real_data_target(prediction.size(0))) error.backward() # Update weights with gradients optimizer.step() # Return error return error . num_test_samples = 16 test_noise = noise(num_test_samples) . logger = Logger(model_name=&#39;VGAN&#39;, data_name=&#39;MNIST&#39;) for epoch in range(epochs): for n_batch, (real_batch,_) in enumerate(data_loader): # 1. Train Discriminator real_data = Variable(image_to_vector(real_batch)) if torch.cuda.is_available(): real_data = real_data.cuda() # Generate fake data fake_data = generator(noise(real_data.size(0))).detach() # Train D d_error, d_pred_real, d_pred_fake = train_discriminator(d_optimizer, real_data, fake_data) # 2. Train Generator # Generate fake data fake_data = generator(noise(real_batch.size(0))) # Train G g_error = train_generator(g_optimizer, fake_data) # Log error logger.log(d_error, g_error, epoch, n_batch, num_batches) # Display Progress if (n_batch) % 100 == 0: display.clear_output(True) # Display Images test_images = vector_to_image(generator(test_noise)).data.cpu() logger.log_images(test_images, num_test_samples, epoch, n_batch, num_batches); # Display status Logs logger.display_status( epoch, epochs, n_batch, num_batches, d_error, g_error, d_pred_real, d_pred_fake ) # Model Checkpoints logger.save_models(generator, discriminator, epoch) . Epoch: [40/200], Batch Num: [100/600] Discriminator Loss: 1.0404, Generator Loss: 1.2824 D(x): 0.6750, D(G(z)): 0.3861 .",
            "url": "https://saiamrit.github.io/technical-blog/2020/12/13/gan.html",
            "relUrl": "/2020/12/13/gan.html",
            "date": " • Dec 13, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "Sai Amrit Patnaik",
          "content": "M.S by Research, IIIT Hyderabad . . Hi! I’m currently an M.S. (Research, Computer Science) student at IIIT Hyderabad. Until recently, I was working as a Research Associate at Centre for Visual Information and Technology (CVIT), working on Face Biometrics using Computer Vision and Deep Learning. I finished my undergraduate from IIIT Bhubanbeswar(IIIT-BH) in 2019, with a degree in Computer Science and Engineering. I have been lucky to receive mentorship by some amazing people including Dr. Anoop Namboodri, Dr. Avinash Sharma, Dr. Aurobindo Routray(IIT-Kharagpur), and Dr. Manish Gupta(Director, Google Research, India). I am incredibly thankful to my collaborators and mentors, and enjoy exploring new domains through collaborations. If you have questions or would like to work together, feel free to reach out through email! . Research Interests . I love to explore different problems, especially in computer vision, Mathematics behind them and experiment with different domains frequently. Successful or not, I try to learn something out of them. I try to share my learnings and experiences on my blog. When I’m not doing academic stuff, I may be spotted in a peaceful corner reading some book, playing my keyboard, or taking naps! Have a look at my CV for more details! .",
          "url": "https://saiamrit.github.io/technical-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "",
          "content": "Posts .",
          "url": "https://saiamrit.github.io/technical-blog/",
          "relUrl": "/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://saiamrit.github.io/technical-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}